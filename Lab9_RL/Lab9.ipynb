{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 9: Reinforcement Learning\n",
    "\n",
    "- Saranpat Funkaew, 58070501068\n",
    "- Chatthong Rimthong, 58070501011"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ในแลปนี้จะเป็นการสร้าง Model ให้สามารถเรียนรู้ด้วยตัวเองได้โดยไม่มี คำตอบ (Label) ให้ในการ train แต่จะเป็นการใช้รางวัล (Reward) ให้กับตัว Model แทน หรือเรียกว่า Reinforcement Learning \n",
    "\n",
    "ซึ่งจะมี Reference ของการทำแลปนี้ https://towardsdatascience.com/reinforcement-learning-w-keras-openai-dqns-1eed3a5338c?fbclid=IwAR3grzzDBY1ZDTfeydV7VEqH9s-pFZCqwQlES5xm4uHGcZzEnVZJMXnA8eQ \n",
    "ในเว็บไซด์เขาได้ทำการสร้าง Model มาเพื่อเล่นเกม \"MountainCar-v0\" ซึ่งเป็นเกมสำหรับฝึก AI โดยจะสร้างเกมจาก library gym และใน gym ก็ยังมีอีกหลายเกมให้ลองเล่นเช่นกัน "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เริ่มจากสร้าง class DQN(Deep Q Network) หรือก็คือ Agent สำหรับการแก้ปัญหานั้น ๆ ใน Environment โดยใน class จะประกอบด้วย variable 10 ตัว<br>\n",
    "1. env: หรือก็คือ Environment (ปัญหาที่ต้องการแก้) ซึ่งในที่นี้ก็คือ การทำให้รถวิ่งขึ้นไปสู่เส้นชัยให้ได้ (MountainCar-v0)\n",
    "2. memory: คือตัวแปรสำหรับเก็บว่าในแต่ละครั้งที่ทดลองนั้นมีผลเป็นอย่างไร (state, action, reward, new_state, done)\n",
    "3. gamma\n",
    "4. epsilon\n",
    "5. epsilon_min\n",
    "6. epsilon_decay\n",
    "7. learning_rate\n",
    "8. tau\n",
    "9. model: สำหรับ predict ว่าจะต้องทำ Action อะไร\n",
    "10. target_model: สำหรับ predict ว่า Model นั้นจะต้อง Take Action อะไร\n",
    "\n",
    "*ที่จำเป็นต้องใช้ 2 Model เพราะถ้าใช้ Model เพียงอย่าเดียวจะทำให้ดป้าหมายนั้นเปลี่ยนไปเรื่อย ๆ  จึงต้องใช้ 2 Model เพื่อให้เป้าหมายนั้นคงที่เป็นเหมือนเดิม"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ในส่วนของ Function จะมี\n",
    "1. create_model: สำหรับสร้าง Model หรือกำหนดว่า network นั้นมี layer อะไรบ้าง\n",
    "2. act: สำหรับทำอะไรบางอย่างกับ Env ซึ่งในที่นี้คือการ predict ว่า Action ต่อไปควรจะเป็นอย่างไร\n",
    "3. remember: คือการจำค่าที่ input และ output ออกมาจาก env \n",
    "4. replay: สำหรับการ train model\n",
    "5. target_train: สำหรับการ train target_model\n",
    "6. save_model: Save weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, env):\n",
    "        self.env     = env\n",
    "        self.memory  = deque(maxlen=2000)\n",
    "        \n",
    "        self.gamma = 0.85\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.005\n",
    "        self.tau = .125\n",
    "\n",
    "        self.model        = self.create_model()\n",
    "        self.target_model = self.create_model()\n",
    "\n",
    "    def create_model(self):\n",
    "        model   = Sequential()\n",
    "        state_shape  = self.env.observation_space.shape\n",
    "        model.add(Dense(24, input_dim=state_shape[0], activation=\"relu\"))\n",
    "        model.add(Dense(48, activation=\"relu\"))\n",
    "        model.add(Dense(24, activation=\"relu\"))\n",
    "        model.add(Dense(self.env.action_space.n))\n",
    "        model.compile(loss=\"mean_squared_error\",\n",
    "            optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def act(self, state):\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon)\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        return np.argmax(self.model.predict(state)[0])\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.append([state, action, reward, new_state, done])\n",
    "\n",
    "    def replay(self):\n",
    "        batch_size = 32\n",
    "        if len(self.memory) < batch_size: \n",
    "            return\n",
    "\n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        for sample in samples:\n",
    "            state, action, reward, new_state, done = sample\n",
    "            target = self.target_model.predict(state)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                Q_future = max(self.target_model.predict(new_state)[0])\n",
    "                target[0][action] = reward + Q_future * self.gamma\n",
    "            self.model.fit(state, target, epochs=1, verbose=0)\n",
    "\n",
    "    def target_train(self):\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        for i in range(len(target_weights)):\n",
    "            target_weights[i] = weights[i] * self.tau + target_weights[i] * (1 - self.tau)\n",
    "        self.target_model.set_weights(target_weights)\n",
    "\n",
    "    def save_model(self, fn):\n",
    "        self.model.save(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### สร้าง Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "trials  = 1000\n",
    "trial_len = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### สร้าง Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent = DQN(env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### เริ่มการ Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "โดยในการเทรนถ้า np.random.random() นั้นน้อยกว่าค่า epsilon จะเป็นค่าที่สุ่ม Action จาก function(env.action_space.sample()) ออกมา แต่ถ้ามากกว่าก็จะนำค่าจากที่ model predict มาเป็น Action เพื่อ Train Model ต่อไป"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to complete in trial 0\n",
      "Failed to complete in trial 1\n",
      "Failed to complete in trial 2\n",
      "Failed to complete in trial 3\n",
      "Failed to complete in trial 4\n",
      "Failed to complete in trial 5\n",
      "Failed to complete in trial 6\n",
      "Failed to complete in trial 7\n",
      "Failed to complete in trial 8\n",
      "Failed to complete in trial 9\n",
      "Failed to complete in trial 10\n",
      "Failed to complete in trial 11\n",
      "Failed to complete in trial 12\n",
      "Failed to complete in trial 13\n",
      "Failed to complete in trial 14\n",
      "Failed to complete in trial 15\n",
      "Failed to complete in trial 16\n",
      "Failed to complete in trial 17\n",
      "Failed to complete in trial 18\n",
      "Failed to complete in trial 19\n",
      "Failed to complete in trial 20\n",
      "Failed to complete in trial 21\n",
      "Failed to complete in trial 22\n",
      "Failed to complete in trial 23\n",
      "Failed to complete in trial 24\n",
      "Failed to complete in trial 25\n",
      "Failed to complete in trial 26\n",
      "Failed to complete in trial 27\n",
      "Failed to complete in trial 28\n",
      "Failed to complete in trial 29\n",
      "Failed to complete in trial 30\n",
      "Failed to complete in trial 31\n",
      "Failed to complete in trial 32\n",
      "Failed to complete in trial 33\n",
      "Failed to complete in trial 34\n",
      "Failed to complete in trial 35\n",
      "Failed to complete in trial 36\n",
      "Failed to complete in trial 37\n",
      "Failed to complete in trial 38\n",
      "Failed to complete in trial 39\n",
      "Failed to complete in trial 40\n",
      "Failed to complete in trial 41\n",
      "Failed to complete in trial 42\n",
      "Failed to complete in trial 43\n",
      "Failed to complete in trial 44\n",
      "Failed to complete in trial 45\n",
      "Failed to complete in trial 46\n",
      "Failed to complete in trial 47\n",
      "Failed to complete in trial 48\n",
      "Failed to complete in trial 49\n",
      "Failed to complete in trial 50\n",
      "Failed to complete in trial 51\n",
      "Failed to complete in trial 52\n",
      "Failed to complete in trial 53\n",
      "Failed to complete in trial 54\n",
      "Failed to complete in trial 55\n",
      "Failed to complete in trial 56\n",
      "Failed to complete in trial 57\n",
      "Failed to complete in trial 58\n",
      "Failed to complete in trial 59\n",
      "Failed to complete in trial 60\n",
      "Failed to complete in trial 61\n",
      "Failed to complete in trial 62\n",
      "Failed to complete in trial 63\n",
      "Failed to complete in trial 64\n",
      "Failed to complete in trial 65\n",
      "Failed to complete in trial 66\n",
      "Failed to complete in trial 67\n",
      "Failed to complete in trial 68\n",
      "Failed to complete in trial 69\n",
      "Failed to complete in trial 70\n",
      "Failed to complete in trial 71\n",
      "Failed to complete in trial 72\n",
      "Failed to complete in trial 73\n",
      "Failed to complete in trial 74\n",
      "Failed to complete in trial 75\n",
      "Failed to complete in trial 76\n",
      "Failed to complete in trial 77\n",
      "Failed to complete in trial 78\n",
      "Failed to complete in trial 79\n",
      "Failed to complete in trial 80\n",
      "Failed to complete in trial 81\n",
      "Completed in 82 trials\n"
     ]
    }
   ],
   "source": [
    "for trial in range(trials):\n",
    "    #Reset Environpment(initial)\n",
    "    cur_state = env.reset().reshape(1,2)\n",
    "    for step in range(trial_len):\n",
    "        # env.render(mode = 'rgb_array') # render how it play the game\n",
    "        action = dqn_agent.act(cur_state) # model predict action \n",
    "        new_state, reward, done, _ = env.step(action) # send action to env\n",
    "\n",
    "        # reward = reward if not done else -20\n",
    "        new_state = new_state.reshape(1,2)\n",
    "        dqn_agent.remember(cur_state, action, reward, new_state, done) # Remember input and output\n",
    "\n",
    "        dqn_agent.replay()       # Train model \n",
    "        dqn_agent.target_train() # Train target_model\n",
    "\n",
    "        cur_state = new_state\n",
    "        if done:\n",
    "            break\n",
    "    if step >= 199:\n",
    "        print(\"Failed to complete in trial {}\".format(trial))\n",
    "        if step % 10 == 0:\n",
    "            dqn_agent.save_model(\"trial-{}.model\".format(trial))\n",
    "    else:\n",
    "        print(\"Completed in {} trials\".format(trial))\n",
    "        dqn_agent.save_model(\"success.model\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ซึ่งจากในการ train จะเป็นการ random ทำให้ trial ของการ train แต่ละครั้งนั้นไม่เท่ากันขึ้นอยู่กับว่า random ได้ออกมาดีแค่ไหน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
